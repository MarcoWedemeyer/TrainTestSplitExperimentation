# TrainTestSplitExperimentation

During one of the first courses in my Masters in Data Science I was introduced to the importance of splitting data into training and test data in order to evaluate model performances on unseen data. We spent some time exploring why this practice is important (e.g. models tend to overfit on given data) but little time on the best practical approach. We were given a rule of thumb that an 80-20 train test split was generally a good choice. The justification always being that the more data is in your training data the better the model can learn the underlying relationship. We weren't taught about the consequences of changing this split to other values. As a curious student I ran some rudimentary simulations on toy data sets. My results for the [Boston Housing data set](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) showed that a split of 40-60 would be far better as it achieved similar performance but at a much lower variance given different random states. I wrote up a quick page to show my results and sent it off to the professor for answers. The response was a reiteration of the points made in the lecture and that it would be different for other data sets. I was dissatisfied. 

After taking more courses, including computational statistics where I learned about proper bootstrapping simulations, I gave the question another shot. I explored the relationship between the train test split and the mean RMSE as well as its standard error. After understanding the trade-off, I looked into which characteristics of datasets influenced this trade-off by generating new artificial datasets using sklearns `make_regression` function. The results and insights can be found in two pdf files in this repository and on [LinkedIn](https://www.linkedin.com/in/marco-andre-wedemeyer/).
